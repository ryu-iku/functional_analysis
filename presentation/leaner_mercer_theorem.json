[
    {
        "role": "user",
        "msg": "Explain the proof of Mercer's Theorem"
    },
    {
        "role": "assistant",
        "msg": "Mercer's Theorem is a fundamental result in the theory of Reproducing Kernel Hilbert Spaces (RKHS) and kernel methods in machine learning. It establishes a crucial connection between positive semi-definite kernel functions and feature maps, showing that any such kernel can be expressed as an inner product in some high-dimensional (possibly infinite-dimensional) feature space.\n\nThe theorem, as presented in the provided text (Theorem 2), states:\n\n**Theorem 2 (Mercer\u2019s Theorem (Mercer, 1909))**:\nSuppose $k: [a,b] \\times [a,b] \\to \\mathbb{R}$ is a continuous symmetric positive semi-definite kernel which is bounded: $\\sup_{x,y} k(x,y) < \\infty$.\nAssume the operator $T_k$ takes a function $f(x)$ as its argument and outputs a new function as:\n$T_k f(x) := \\int_a^b k(x,y) f(y) dy$, (Eq. 18)\nwhich is a Fredholm integral equation.\nThen, there is a set of orthonormal bases $\\{\\psi_i(.)\\}_{i=1}^\\infty$ of $L^2(a,b)$ consisting of eigenfunctions of $T_k$ such that the corresponding sequence of eigenvalues $\\{\\lambda_i\\}_{i=1}^\\infty$ are non-negative:\n$\\int k(x,y) \\psi_i(y) dy = \\lambda_i \\psi_i(x)$. (Eq. 20)\nThe eigenfunctions corresponding to the non-zero eigenvalues are continuous on $[a,b]$ and $k$ can be represented as:\n$k(x,y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y)$, (Eq. 21)\nwhere the convergence is absolute and uniform.\n\n---\n\n### Proof Explanation (following the provided text)\n\nThe proof presented in the text outlines four main steps to establish these conclusions.\n\n**1. Properties of the Integral Operator ($T_k$): Symmetric and Compact**\n\n*   **Symmetry:** The kernel $k(x,y)$ is assumed to be symmetric, meaning $k(x,y) = k(y,x)$. This property of the kernel directly implies that the integral operator $T_k$ is a **symmetric operator** on the $L^2(a,b)$ space. (A linear operator $A$ on a Hilbert space $H$ is symmetric if $\\langle Af, g \\rangle = \\langle f, Ag \\rangle$ for all $f, g \\in H$. For $T_k$, this means $\\int_a^b (T_k f(x)) g(x) dx = \\int_a^b f(x) (T_k g(x)) dx$, which holds due to the symmetry of $k(x,y)$ and Fubini's theorem).\n*   **Compactness:** The theorem assumes the kernel $k(x,y)$ is continuous and bounded: $\\sup_{x,y} k(x,y) < \\infty$. Given this, the text states that the integral operator $T_k$ is a **compact operator**. This relies on the Arzel\u00e0-Ascoli theorem, which implies that the image of a bounded set (like the unit ball in $L^2(a,b)$) under $T_k$ is relatively compact. In essence, compact operators \"smooth out\" functions and map them into a smaller, more \"well-behaved\" space.\n\n**2. Application of the Spectral Theorem**\n\n*   Since $T_k$ is a **symmetric and compact operator** on the Hilbert space $L^2(a,b)$, the **Spectral Theorem for compact self-adjoint operators** can be applied (as referenced in the text, Hawkins, 1975).\n*   This theorem guarantees the existence of an orthonormal basis $\\{\\psi_i(.)\\}_{i=1}^\\infty$ for $L^2(a,b)$ consisting of eigenfunctions of $T_k$. Each eigenfunction $\\psi_i(x)$ has a corresponding real eigenvalue $\\lambda_i$.\n*   The relationship is expressed as $T_k \\psi_i(x) = \\lambda_i \\psi_i(x)$ (Eq. 22).\n*   Substituting the definition of $T_k$ (Eq. 18) into this eigen-equation yields:\n    $\\int_a^b k(x,y) \\psi_i(y) dy = \\lambda_i \\psi_i(x)$ (Eq. 23, which proves Eq. 20).\n*   Furthermore, the positive semi-definiteness of the kernel $k(x,y)$ (meaning $\\int\\int k(x,y) f(x) f(y) dx dy \\ge 0$ for all $f$) implies that all eigenvalues $\\lambda_i$ must be non-negative.\n\n**3. Derivation of the Kernel Expansion (Mercer's Series)**\n\n*   The core idea here is to express the kernel function $k(x,y)$ itself in terms of its eigenfunctions. Since $\\{\\psi_i(.)\\}_{i=1}^\\infty$ forms a complete orthonormal basis for $L^2(a,b)$, any function in this space can be expanded as a linear combination of these basis functions.\n*   Consider $k(x,y)$ as a function of $y$ for a fixed $x$. Let $g(y) = k(x,y)$. Since $k(x,y)$ is continuous and bounded, $g(y) \\in L^2(a,b)$.\n*   We can expand $g(y)$ in the orthonormal basis $\\{\\psi_i(y)\\}_{i=1}^\\infty$:\n    $k(x,y) = \\sum_{i=1}^\\infty \\langle k(x,\\cdot), \\psi_i \\rangle_{L^2} \\psi_i(y)$.\n    (Here, $\\langle \\cdot, \\cdot \\rangle_{L^2}$ denotes the inner product in $L^2(a,b)$ with respect to the $y$ variable).\n*   The coefficients of this expansion are given by:\n    $\\langle k(x,\\cdot), \\psi_i \\rangle_{L^2} = \\int_a^b k(x,y) \\psi_i(y) dy$.\n*   From Step 2 (Eq. 20), we know that this integral is equal to $\\lambda_i \\psi_i(x)$.\n*   Substituting these coefficients back into the expansion, we get:\n    $k(x,y) = \\sum_{i=1}^\\infty (\\lambda_i \\psi_i(x)) \\psi_i(y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y)$. (Eq. 27, which proves Eq. 21).\n\n**4. Proving Absolute and Uniform Convergence**\n\n*   This step addresses the nature of the convergence of the series in Eq. (21). The text introduces a \"truncated kernel\" $r_n(x,y) = k(x,y) - \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(y)$.\n*   Because $k(x,y)$ is positive semi-definite and the eigenvalues $\\lambda_i$ are non-negative, the remaining kernel $r_n(x,y)$ is also positive semi-definite. This implies $r_n(x,x) \\ge 0$.\n*   Therefore, for any $x \\in [a,b]$:\n    $k(x,x) - \\sum_{i=1}^n \\lambda_i \\psi_i(x)^2 \\ge 0 \\implies \\sum_{i=1}^n \\lambda_i \\psi_i(x)^2 \\le k(x,x)$.\n*   Since $k(x,y)$ is bounded (Eq. 17), $k(x,x)$ is also bounded. This means the partial sums $\\sum_{i=1}^n \\lambda_i \\psi_i(x)^2$ are bounded.\n*   The text then uses the Cauchy-Schwarz inequality for the series $\\sum \\lambda_i \\psi_i(x) \\psi_i(y)$ to show that its partial sums are bounded, implying absolute convergence.\n*   The uniform convergence follows from the continuity of $k(x,y)$ and the continuity of the eigenfunctions corresponding to non-zero eigenvalues, combined with results from the theory of integral equations. This ensures that the series converges uniformly for all $(x,y)$ in the domain.\n\n---\n\n### Significance of Mercer's Theorem\n\nMercer's Theorem is profoundly important for several reasons, especially in machine learning:\n\n1.  **Existence of Feature Map:** It guarantees that for any valid (symmetric, positive semi-definite) kernel $k(x,y)$, there exists a feature map $\\varphi(x)$ such that $k(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$ (as shown in Eq. 34 of the text, by defining $\\varphi(x) = [\\sqrt{\\lambda_1}\\psi_1(x), \\sqrt{\\lambda_2}\\psi_2(x), \\ldots]^\\top$). This provides the theoretical foundation for the \"kernel trick.\"\n2.  **RKHS Connection:** It links a positive semi-definite kernel directly to a Reproducing Kernel Hilbert Space (RKHS). The space spanned by the eigenfunctions (weighted by eigenvalues) forms an RKHS where the kernel acts as a reproducing kernel.\n3.  **Kernel Trick Justification:** The theorem justifies the \"kernel trick\" in machine learning, where complex non-linear operations in a high-dimensional feature space can be implicitly computed using only the kernel function in the original input space, avoiding explicit computation of the feature map $\\varphi(x)$.\n4.  **Spectral Decomposition:** It provides a spectral decomposition of the kernel function itself, which is crucial for understanding its properties and for methods like Kernel PCA and other spectral learning algorithms."
    },
    {
        "role": "user",
        "msg": "Explain Fubini's theorem"
    },
    {
        "role": "assistant",
        "msg": "Fubini's Theorem is a powerful result in real analysis that provides conditions under which the order of integration in a multiple integral can be interchanged without changing the result. It is fundamental for working with functions of several variables and is widely used in probability theory, functional analysis, and applied mathematics.\n\n### Intuition\n\nImagine you want to calculate the volume under a surface $f(x,y)$ over a rectangular region in the $xy$-plane. You can do this in two ways:\n1.  **Slice parallel to the $y$-axis:** For each $x$, you integrate $f(x,y)$ with respect to $y$ (finding the area of a cross-section), and then you integrate these areas with respect to $x$. This gives $\\int_a^b \\left( \\int_c^d f(x,y) dy \\right) dx$.\n2.  **Slice parallel to the $x$-axis:** For each $y$, you integrate $f(x,y)$ with respect to $x$, and then you integrate these areas with respect to $y$. This gives $\\int_c^d \\left( \\int_a^b f(x,y) dx \\right) dy$.\n\nFubini's Theorem states that, under certain conditions, these two iterated integrals (and the full double integral) will yield the same result.\n\n### Formal Statement\n\nLet $f(x,y)$ be a measurable function defined on a product space $X \\times Y$ (e.g., a rectangular region $[a,b] \\times [c,d]$ in $\\mathbb{R}^2$, or more generally, a product of two measure spaces).\n\n**Fubini's Theorem states that if $f(x,y)$ is absolutely integrable over $X \\times Y$, i.e., if:**\n$$ \\iint_{X \\times Y} |f(x,y)| \\, d(x,y) < \\infty $$\n**then the following equalities hold:**\n$$ \\iint_{X \\times Y} f(x,y) \\, d(x,y) = \\int_X \\left( \\int_Y f(x,y) \\, dy \\right) dx = \\int_Y \\left( \\int_X f(x,y) \\, dx \\right) dy $$\nwhere the integrals are Lebesgue integrals.\n\n**Important Notes on Conditions:**\n*   **Absolute Integrability:** The absolute integrability condition is crucial. If a function is not absolutely integrable (e.g., like some improper Riemann integrals), interchanging the order of integration can lead to different results or even non-existence of one of the iterated integrals.\n*   **Non-negative Functions:** If $f(x,y) \\ge 0$ for all $(x,y)$, then the theorem holds automatically, as the absolute integrability condition is equivalent to the integrability of $f(x,y)$ itself.\n*   **Measurability:** The function $f$ must be measurable with respect to the product measure on $X \\times Y$. This is a technical condition usually met by continuous functions or functions commonly encountered in applications.\n\n### Importance and Usefulness\n\n1.  **Simplification of Calculations:** Fubini's Theorem allows us to choose the most convenient order of integration for calculating multiple integrals, often simplifying complex computations.\n2.  **Theoretical Tool:** It is a fundamental tool in proofs across various fields of mathematics, enabling manipulations of integrals that are essential for establishing deeper results.\n\n### Connection to Mercer's Theorem (as per text)\n\nIn the proof of Mercer's Theorem, Fubini's Theorem is implicitly used in **Step 1** to show the **symmetry of the Hilbert-Schmidt integral operator $T_k$**.\n\nThe definition of a symmetric operator $T_k$ on $L^2(a,b)$ requires that for any two functions $f, g \\in L^2(a,b)$:\n$$ \\langle T_k f, g \\rangle_{L^2} = \\langle f, T_k g \\rangle_{L^2} $$\nLet's expand the left side using the definition of the inner product in $L^2$ (Eq. 75 in the text, assuming real functions) and the operator $T_k$ (Eq. 18):\n$$ \\langle T_k f, g \\rangle_{L^2} = \\int_a^b (T_k f(x)) g(x) \\, dx = \\int_a^b \\left( \\int_a^b k(x,y) f(y) \\, dy \\right) g(x) \\, dx $$\nThis is an iterated integral. To show symmetry, we want to rearrange it. If we can swap the order of integration, we get:\n$$ \\int_a^b \\int_a^b k(x,y) f(y) g(x) \\, dy \\, dx \\quad \\xrightarrow{\\text{Fubini's Theorem}} \\quad \\int_a^b \\int_a^b k(x,y) f(y) g(x) \\, dx \\, dy $$\nNow, using the symmetry property of the kernel $k(x,y) = k(y,x)$ (which is an assumption of Mercer's Theorem):\n$$ \\int_a^b \\int_a^b k(y,x) f(y) g(x) \\, dx \\, dy $$\nFinally, by rearranging the terms and recognizing the structure of $T_k$:\n$$ \\int_a^b f(y) \\left( \\int_a^b k(y,x) g(x) \\, dx \\right) \\, dy = \\int_a^b f(y) (T_k g(y)) \\, dy = \\langle f, T_k g \\rangle_{L^2} $$\nThis demonstrates that $\\langle T_k f, g \\rangle_{L^2} = \\langle f, T_k g \\rangle_{L^2}$, thus proving the symmetry of $T_k$.\n\n**Why Fubini's conditions are met here:**\nThe function being integrated is $F(x,y) = k(x,y) f(y) g(x)$.\n*   The kernel $k(x,y)$ is assumed to be continuous and bounded (Eq. 17).\n*   The functions $f(y)$ and $g(x)$ are elements of $L^2(a,b)$, meaning $\\int_a^b |f(y)|^2 dy < \\infty$ and $\\int_a^b |g(x)|^2 dx < \\infty$.\n*   The product of bounded continuous functions and $L^2$ functions over a finite domain $[a,b] \\times [a,b]$ ensures that $F(x,y)$ is absolutely integrable, satisfying the conditions for Fubini's Theorem to apply. This allows the crucial step of interchanging the order of integration."
    },
    {
        "role": "user",
        "msg": "Explain  Arzel\u00e0-Ascoli theorem"
    },
    {
        "role": "assistant",
        "msg": "The Arzel\u00e0-Ascoli Theorem is a fundamental result in real analysis and functional analysis that provides conditions under which a sequence of continuous functions on a compact set will have a uniformly convergent subsequence. It is a powerful tool for proving the existence of solutions to differential equations, properties of integral operators, and other results involving function spaces.\n\n### High-Level Intuition\n\nIn finite-dimensional Euclidean space, a set is \"compact\" (meaning every sequence in the set has a convergent subsequence) if and only if it is closed and bounded (Heine-Borel theorem). However, in infinite-dimensional spaces, such as spaces of functions, being closed and bounded is not enough to guarantee compactness. Functions can be bounded but still \"wiggle\" infinitely fast, preventing convergence.\n\nThe Arzel\u00e0-Ascoli Theorem provides the *additional conditions* needed for a set of functions to be relatively compact (meaning its closure is compact) in a space of continuous functions. These conditions essentially limit how \"wiggly\" or \"spread out\" the functions in the set can be.\n\n### Formal Statement (Simplified for Context)\n\nLet $X$ be a compact metric space (e.g., a closed and bounded interval like $[a,b]$ in $\\mathbb{R}$). Let $\\mathcal{F}$ be a family (set) of continuous functions from $X$ to $\\mathbb{R}$ (or $\\mathbb{C}$, or $\\mathbb{R}^n$).\n\nThe Arzel\u00e0-Ascoli Theorem states that $\\mathcal{F}$ is **relatively compact** in the space of continuous functions $C(X)$ (equipped with the uniform norm, $\\|f\\|_\\infty = \\sup_{x \\in X} |f(x)|$) if and only if it satisfies two conditions:\n\n1.  **Pointwise Boundedness:** For each point $x \\in X$, the set of values $\\{f(x) : f \\in \\mathcal{F}\\}$ is bounded. That is, for every $x \\in X$, there exists a constant $M_x$ such that $|f(x)| \\le M_x$ for all $f \\in \\mathcal{F}$.\n    *   *(Often, in practical applications, a stronger condition called **uniform boundedness** is used: there exists a single constant $M$ such that $|f(x)| \\le M$ for all $f \\in \\mathcal{F}$ and all $x \\in X$. For continuous functions on a compact domain, pointwise boundedness implies uniform boundedness.)*\n\n2.  **Equicontinuity:** The family $\\mathcal{F}$ is equicontinuous. This means that all functions in the family are \"equally continuous\" at every point. Formally, for every $\\epsilon > 0$ and every $x \\in X$, there exists a $\\delta > 0$ such that for all $f \\in \\mathcal{F}$ and all $x' \\in X$ with $d(x, x') < \\delta$, we have $|f(x) - f(x')| < \\epsilon$.\n    *   *(The key here is that $\\delta$ depends only on $\\epsilon$ and $x$, not on the individual function $f$. It ensures that no function in the family can vary too rapidly.)*\n\nIf these two conditions are met, then every sequence of functions in $\\mathcal{F}$ has a subsequence that converges uniformly to a continuous function.\n\n### Relevance to Mercer's Theorem (as per text)\n\nIn **Step 1 of the proof of Mercer's Theorem**, the Arzel\u00e0-Ascoli Theorem is invoked to establish that the Hilbert-Schmidt integral operator $T_k$ is a **compact operator**.\n\nThe statement in the text is: \"Therefore, according to the Arzel\u00e0-Ascoli theorem (Arzel\u00e0, 1895), the image of the unit ball after applying the operator is compact. In other words, the operator $T_k$ is compact.\"\n\nLet's break down how this connection works:\n\n1.  **The Operator $T_k$:**\n    $T_k f(x) := \\int_a^b k(x,y) f(y) \\, dy$ (Eq. 18).\n    This operator maps functions $f \\in L^2([a,b])$ to functions $T_k f \\in C([a,b])$ (the space of continuous functions on $[a,b]$), provided $k(x,y)$ is continuous.\n\n2.  **Compact Operator Definition:** A linear operator $A: V \\to W$ (between normed spaces) is called **compact** if it maps bounded sets in $V$ to relatively compact sets in $W$. In the context of Mercer's Theorem, $V = L^2([a,b])$ and $W = C([a,b])$. We want to show that the image of the unit ball in $L^2([a,b])$ (i.e., the set $\\{T_k f \\mid \\|f\\|_{L^2} \\le 1\\}$) is relatively compact in $C([a,b])$.\n\n3.  **Applying Arzel\u00e0-Ascoli:** To show that the set $\\mathcal{F} = \\{T_k f \\mid \\|f\\|_{L^2} \\le 1\\}$ is relatively compact in $C([a,b])$, we need to verify the two conditions of Arzel\u00e0-Ascoli:\n\n    *   **Uniform Boundedness:** We need to show that there is a constant $M$ such that for all $f$ with $\\|f\\|_{L^2} \\le 1$, and for all $x \\in [a,b]$, $|T_k f(x)| \\le M$.\n        *   $|T_k f(x)| = \\left| \\int_a^b k(x,y) f(y) \\, dy \\right|$\n        *   By Cauchy-Schwarz inequality: $\\le \\left( \\int_a^b |k(x,y)|^2 \\, dy \\right)^{1/2} \\left( \\int_a^b |f(y)|^2 \\, dy \\right)^{1/2}$\n        *   Since $\\|f\\|_{L^2} \\le 1$, this becomes: $\\le \\left( \\int_a^b |k(x,y)|^2 \\, dy \\right)^{1/2} \\cdot 1$\n        *   The kernel $k(x,y)$ is assumed to be **continuous** on the compact set $[a,b] \\times [a,b]$, so it is bounded (Eq. 17: $\\sup_{x,y} k(x,y) < \\infty$). Let $K_{max} = \\sup_{x,y} |k(x,y)|$.\n        *   Then $\\int_a^b |k(x,y)|^2 \\, dy \\le \\int_a^b K_{max}^2 \\, dy = K_{max}^2 (b-a)$.\n        *   So, $|T_k f(x)| \\le K_{max} \\sqrt{b-a}$. This shows that the family $\\{T_k f\\}$ is uniformly bounded.\n\n    *   **Equicontinuity:** We need to show that for any $\\epsilon > 0$, there exists a $\\delta > 0$ such that for all $f$ with $\\|f\\|_{L^2} \\le 1$, and for any $x_1, x_2 \\in [a,b]$ with $|x_1 - x_2| < \\delta$, we have $|T_k f(x_1) - T_k f(x_2)| < \\epsilon$.\n        *   $|T_k f(x_1) - T_k f(x_2)| = \\left| \\int_a^b (k(x_1,y) - k(x_2,y)) f(y) \\, dy \\right|$\n        *   By Cauchy-Schwarz inequality: $\\le \\left( \\int_a^b |k(x_1,y) - k(x_2,y)|^2 \\, dy \\right)^{1/2} \\left( \\int_a^b |f(y)|^2 \\, dy \\right)^{1/2}$\n        *   Again, since $\\|f\\|_{L^2} \\le 1$, this simplifies to: $\\le \\left( \\int_a^b |k(x_1,y) - k(x_2,y)|^2 \\, dy \\right)^{1/2}$\n        *   Since $k(x,y)$ is **continuous** on the compact set $[a,b] \\times [a,b]$, it is **uniformly continuous**. This means that for any $\\eta > 0$, there exists a $\\delta > 0$ such that if $|x_1 - x_2| < \\delta$, then $|k(x_1,y) - k(x_2,y)| < \\eta$ for *all* $y \\in [a,b]$.\n        *   So, if $|x_1 - x_2| < \\delta$, then $\\int_a^b |k(x_1,y) - k(x_2,y)|^2 \\, dy < \\int_a^b \\eta^2 \\, dy = \\eta^2 (b-a)$.\n        *   Thus, $|T_k f(x_1) - T_k f(x_2)| < \\eta \\sqrt{b-a}$. By choosing $\\eta$ appropriately (e.g., $\\eta = \\epsilon/\\sqrt{b-a}$), we can make this difference less than $\\epsilon$. Crucially, $\\delta$ depends only on $\\epsilon$ (and the uniform continuity of $k$), not on $f$. This demonstrates equicontinuity.\n\nSince both uniform boundedness and equicontinuity are satisfied, the Arzel\u00e0-Ascoli Theorem ensures that the image of the unit ball under $T_k$ is relatively compact in $C([a,b])$. This implies that $T_k$ is a compact operator. The compactness of $T_k$ is a critical prerequisite for applying the Spectral Theorem for compact self-adjoint operators in the next step of Mercer's proof."
    },
    {
        "role": "user",
        "msg": "Explain the spectral theorem and \"spectral decomposition\""
    },
    {
        "role": "assistant",
        "msg": "The Spectral Theorem is a profound result in linear algebra and functional analysis that generalizes the concept of diagonalizing a matrix to a broader class of linear operators, particularly those acting on Hilbert spaces. It establishes that certain types of operators can be represented in a very simple form using their eigenvalues and eigenvectors (or eigenfunctions in infinite-dimensional spaces).\n\n### Spectral Theorem for Matrices (Finite-Dimensional Case)\n\nLet's start with the more familiar finite-dimensional case for matrices, which provides a strong intuition for the general theorem.\n\n**Statement:** For a **symmetric real matrix** $A$ (or a Hermitian complex matrix $A^* = A$), there exists an orthonormal basis of eigenvectors for the vector space.\nThis means that:\n1.  All eigenvalues of $A$ are real numbers.\n2.  The eigenvectors corresponding to distinct eigenvalues are orthogonal.\n3.  $A$ is diagonalizable by an orthogonal (or unitary) matrix.\n\n**Spectral Decomposition:**\nThe theorem allows us to express the matrix $A$ in terms of its eigenvalues and eigenvectors. If $A$ is an $n \\times n$ symmetric matrix with eigenvalues $\\lambda_1, \\ldots, \\lambda_n$ and corresponding orthonormal eigenvectors $v_1, \\ldots, v_n$:\n\n*   **Matrix Form:** $A = V \\Lambda V^T$\n    where $V$ is an orthogonal matrix whose columns are the eigenvectors ($V = [v_1 | \\ldots | v_n]$), and $\\Lambda$ is a diagonal matrix whose diagonal entries are the eigenvalues ($\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)$). Since $V$ is orthogonal, $V^T = V^{-1}$.\n\n*   **Sum of Outer Products Form:** $A = \\sum_{i=1}^n \\lambda_i v_i v_i^T$\n    This form explicitly shows how the matrix $A$ can be \"decomposed\" into a weighted sum of rank-one projection matrices ($v_i v_i^T$), where the weights are the eigenvalues. Each $v_i v_i^T$ projects a vector onto the direction of $v_i$.\n\n### Spectral Theorem for Operators (Infinite-Dimensional Case)\n\nThe version of the Spectral Theorem relevant to Mercer's Theorem extends this idea to linear operators on infinite-dimensional Hilbert spaces. The most common version used in this context is the **Spectral Theorem for compact self-adjoint operators**.\n\n**Statement:** Let $H$ be a Hilbert space (e.g., $L^2([a,b])$). Let $T: H \\to H$ be a **compact and self-adjoint linear operator**. Then:\n1.  All eigenvalues of $T$ are real.\n2.  The set of eigenvalues is discrete (can be finite or countably infinite) and can only accumulate at zero. If there are infinitely many eigenvalues, they converge to zero.\n3.  There exists an orthonormal basis $\\{\\psi_i\\}_{i=1}^\\infty$ of $H$ consisting of eigenvectors (often called **eigenfunctions** in function spaces) of $T$.\n4.  For any $f \\in H$, $T$ can be represented as:\n    $Tf = \\sum_{i=1}^\\infty \\lambda_i \\langle f, \\psi_i \\rangle \\psi_i$\n    where $\\lambda_i$ are the eigenvalues corresponding to the eigenfunctions $\\psi_i$, and $\\langle \\cdot, \\cdot \\rangle$ is the inner product in $H$.\n    This is the **spectral decomposition** of the operator $T$.\n\n**Key Conditions Explained:**\n*   **Hilbert Space:** A complete inner product space. This ensures that notions of distance, angle, and convergence are well-defined.\n*   **Linear Operator:** $T(af + bg) = aTf + bTg$.\n*   **Self-adjoint (or Symmetric):** For real Hilbert spaces, this means $\\langle Tf, g \\rangle = \\langle f, Tg \\rangle$ for all $f, g \\in H$. This is the generalization of a symmetric matrix ($A^T = A$).\n*   **Compact Operator:** This is the crucial condition for infinite-dimensional spaces. A compact operator maps bounded sets to relatively compact sets. Intuitively, it \"smooths out\" functions, making them \"less wiggly\" and ensuring that the image of a bounded set of functions has a convergent subsequence (via Arzel\u00e0-Ascoli theorem). Without compactness, eigenvalues might not form a discrete set or converge to zero.\n\n### Relevance to Mercer's Theorem\n\nIn **Step 2 of the proof of Mercer's Theorem**, the Spectral Theorem for compact self-adjoint operators is directly applied to the Hilbert-Schmidt integral operator $T_k$:\n\n$$ T_k f(x) := \\int_a^b k(x,y) f(y) \\, dy \\quad \\text{(Eq. 18 in text)} $$\n\nThe previous steps of Mercer's proof establish the conditions necessary for the Spectral Theorem to apply to $T_k$:\n\n1.  **Symmetry:** The text states that $T_k$ is a symmetric operator on $L^2(a,b)$ because the kernel $k(x,y)$ is symmetric (an assumption of Mercer's Theorem), and Fubini's theorem allows interchanging integration order to show $\\langle T_k f, g \\rangle = \\langle f, T_k g \\rangle$.\n2.  **Compactness:** The text explains that because $k(x,y)$ is continuous and bounded on $[a,b] \\times [a,b]$ (assumptions of Mercer's Theorem), the Arzel\u00e0-Ascoli theorem implies that $T_k$ is a compact operator.\n\nOnce these conditions (symmetric and compact) are established, the Spectral Theorem immediately guarantees:\n\n*   The existence of an orthonormal basis of eigenfunctions $\\{\\psi_i(.)\\}_{i=1}^\\infty$ for $L^2(a,b)$.\n*   That these eigenfunctions satisfy the eigenvalue equation: $\\int_a^b k(x,y) \\psi_i(y) \\, dy = \\lambda_i \\psi_i(x)$ (Eq. 20 and 23 in text).\n*   That the eigenvalues $\\lambda_i$ are real and, due to the positive semi-definiteness of the kernel, must be non-negative.\n*   If there are infinitely many eigenvalues, they converge to zero.\n\nThis result from the Spectral Theorem is then used in Step 3 of Mercer's proof to derive the famous Mercer's series expansion of the kernel function itself (Eq. 21): $k(x,y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y)$.\n\n### Why \"Spectral\"?\n\nThe term \"spectrum\" in mathematics refers to the set of eigenvalues of an operator. The Spectral Theorem is called \"spectral\" because it provides a way to understand the operator by decomposing it into components related to its eigenvalues, which form its spectrum. In essence, it tells us that certain operators can be completely understood by their action on their \"special directions\" (eigenvectors/eigenfunctions) and the scaling factors (eigenvalues) associated with them."
    },
    {
        "role": "user",
        "msg": "Explain Parseval's theorem and the Bessel's inequality"
    },
    {
        "role": "assistant",
        "msg": "The concepts of Bessel's Inequality and Parseval's Theorem are fundamental in the study of Hilbert spaces, providing crucial insights into how vectors (or functions) can be represented and approximated using orthonormal systems. They are particularly important in fields like signal processing, quantum mechanics, and functional analysis, including the proof of Mercer's Theorem.\n\n### Bessel's Inequality\n\n**Intuition:** Imagine you have a vector in a high-dimensional space, and you want to project it onto a *subset* of orthogonal directions. The squared length of the original vector will always be greater than or equal to the sum of the squared lengths of its projections onto these chosen directions. You can't \"gain\" energy or length by projecting onto a subspace; you can only lose some or preserve it if the vector lies entirely within that subspace.\n\n**Formal Statement:**\nLet $H$ be a Hilbert space (a complete inner product space). Let $\\{\\phi_i\\}_{i=1}^N$ be an **orthonormal system** in $H$. This means:\n1.  $\\langle \\phi_i, \\phi_j \\rangle = 0$ for $i \\ne j$ (orthogonality).\n2.  $\\|\\phi_i\\|^2 = \\langle \\phi_i, \\phi_i \\rangle = 1$ for all $i$ (normality).\nThe system can be finite ($N$ is a positive integer) or countably infinite ($N=\\infty$).\n\nFor any vector $f \\in H$, Bessel's Inequality states:\n$$ \\sum_{i=1}^N |\\langle f, \\phi_i \\rangle|^2 \\le \\|f\\|^2 $$\nwhere $\\langle f, \\phi_i \\rangle$ are the scalar projection coefficients of $f$ onto $\\phi_i$, and $\\|f\\|^2 = \\langle f, f \\rangle$ is the squared norm (or \"energy\") of $f$.\n\n**Key Implication:** The sum of the squares of the projection coefficients is bounded by the squared norm of the original vector. If the sum is finite, the coefficients must eventually become small.\n\n### Parseval's Theorem (or Parseval's Identity)\n\n**Intuition:** Parseval's Theorem is a special case where Bessel's Inequality becomes an *equality*. This happens when the orthonormal system is actually a **complete orthonormal basis** for the entire Hilbert space. In this scenario, projecting onto the basis vectors captures *all* the \"information\" or \"energy\" of the original vector. No part of the vector is left unprojected.\n\n**Formal Statement:**\nLet $H$ be a Hilbert space. Let $\\{\\phi_i\\}_{i=1}^\\infty$ be a **complete orthonormal basis** (or a maximal orthonormal system) in $H$. This means that not only are the $\\phi_i$ orthonormal, but also any vector $f \\in H$ can be uniquely expressed as an infinite linear combination of these basis vectors:\n$$ f = \\sum_{i=1}^\\infty \\langle f, \\phi_i \\rangle \\phi_i $$\nFor any vector $f \\in H$, Parseval's Theorem states:\n$$ \\sum_{i=1}^\\infty |\\langle f, \\phi_i \\rangle|^2 = \\|f\\|^2 $$\nThis is a direct consequence of the completeness of the basis. If the sum of squared projections is strictly less than $\\|f\\|^2$, it would imply there's a component of $f$ that is orthogonal to all $\\phi_i$, contradicting the completeness of the basis.\n\n**Key Implication:** The \"energy\" or squared norm of a vector is preserved when moving from the original space to the space of its Fourier coefficients (the coefficients $\\langle f, \\phi_i \\rangle$). This is crucial in signal processing, where it implies that the total energy of a signal is the same whether measured in the time domain or the frequency domain.\n\n### Relevance to Mercer's Theorem\n\nIn **Step 3 of the proof of Mercer's Theorem**, the text states: \"According to Parseval\u2019s theorem (Parseval des Chenes, 1806), the Bessel\u2019s inequality can be converted to equality (Saxe, 2002). For the orthonormal bases $\\{\\psi_i(.)\\}_{i=1}^\\infty$ in the Hilbert space $H$ associated with kernel $k$, we have for any function $f \\in L^2(a,b)$: $f = \\sum_{i=1}^\\infty \\langle f, \\psi_i \\rangle_k \\psi_i$ (Eq. 24).\"\n\nHere's how these concepts are applied:\n\n1.  **Complete Orthonormal Basis:** In Step 2 of Mercer's Theorem proof, the Spectral Theorem for compact self-adjoint operators is applied to the integral operator $T_k$. This theorem *guarantees* the existence of an orthonormal basis $\\{\\psi_i(.)\\}_{i=1}^\\infty$ for the Hilbert space $L^2([a,b])$ consisting of eigenfunctions of $T_k$. The crucial aspect here is that it's a **complete** orthonormal basis.\n\n2.  **Function Expansion:** Since $\\{\\psi_i(.)\\}_{i=1}^\\infty$ is a complete orthonormal basis, any function $f \\in L^2([a,b])$ can be perfectly represented (expanded) as a linear combination of these eigenfunctions. This is precisely what Eq. (24) in the text states:\n    $f = \\sum_{i=1}^\\infty \\langle f, \\psi_i \\rangle_k \\psi_i$.\n    The coefficients $\\langle f, \\psi_i \\rangle_k$ represent how much of $f$ lies in the direction of each eigenfunction $\\psi_i$.\n\n3.  **Derivation of Mercer's Series:** This expansion is then used to derive the main result of Mercer's Theorem, the series expansion of the kernel function itself: $k(x,y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y)$.\n    *   The idea is to treat $k(x,y)$ (for a fixed $x$) as a function of $y$, let's call it $g_x(y) = k(x,y)$.\n    *   Since $g_x(y) \\in L^2([a,b])$ and $\\{\\psi_i\\}$ is a complete orthonormal basis, we can write $g_x(y) = \\sum_{i=1}^\\infty \\langle g_x, \\psi_i \\rangle_{L^2} \\psi_i(y)$.\n    *   The coefficients are $\\langle g_x, \\psi_i \\rangle_{L^2} = \\int_a^b k(x,y) \\psi_i(y) dy$.\n    *   From Step 2 (Eq. 20), we know that $\\int_a^b k(x,y) \\psi_i(y) dy = \\lambda_i \\psi_i(x)$.\n    *   Substituting this back into the expansion yields $k(x,y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y)$.\n\nIn summary, Bessel's Inequality gives a bound on the \"energy\" captured by a partial projection, while Parseval's Theorem (which is Bessel's inequality as an equality) asserts that if the projection is onto a *complete* basis, all the \"energy\" is captured, allowing for perfect reconstruction of the original function or vector. This completeness property, guaranteed by the Spectral Theorem, is what allows the expansion of the kernel function in Mercer's Theorem proof."
    },
    {
        "role": "user",
        "msg": "Explain the process from equation (25) to (27)"
    },
    {
        "role": "assistant",
        "msg": "Let's break down the process from equation (25) to (27) in the proof of Mercer's Theorem. This step is crucial as it directly leads to the famous Mercer's series expansion of the kernel function.\n\nWe start with two key equations from the context:\n\n1.  **Equation (18): Definition of the integral operator $T_k$**\n    $T_k f(x) := \\int_a^b k(x,y) f(y) \\, dy$\n    This equation defines how the operator $T_k$ transforms an input function $f(y)$ into an output function $(T_k f)(x)$. The kernel $k(x,y)$ acts as the \"weighting function\" in this integral transformation.\n\n2.  **Equation (25): Spectral decomposition of the operator $T_k$ applied to a function $f$**\n    $T_k f = \\sum_{i=1}^\\infty \\lambda_i \\langle f, \\psi_i \\rangle_k \\psi_i$\n    This equation comes from Step 3 of the proof. It utilizes the fact (from Step 2 and Parseval's Theorem) that any function $f \\in L^2(a,b)$ can be expressed as a sum of the orthonormal eigenfunctions $\\{\\psi_i\\}$ (Eq. 24). When $T_k$ acts on this sum, due to its linearity and the eigenfunction property ($T_k \\psi_i = \\lambda_i \\psi_i$, from Eq. 22), it results in this weighted sum of eigenfunctions. The term $\\langle f, \\psi_i \\rangle_k$ is the inner product of $f$ and $\\psi_i$ in $L^2(a,b)$, which is $\\int_a^b f(y) \\psi_i(y) \\, dy$ (assuming real functions, as is common in this context).\n\nNow, let's follow the logical flow from (25) to (27):\n\n**Step 1: Equate the two expressions for $T_k f(x)$**\n\nBoth Equation (18) and Equation (25) provide an expression for the function $T_k f(x)$. Let's write them both out, explicitly showing the dependence on $x$:\n\nFrom (18):\n$$ (T_k f)(x) = \\int_a^b k(x,y) f(y) \\, dy $$\n\nFrom (25), substituting the definition of the inner product $\\langle f, \\psi_i \\rangle_k = \\int_a^b f(y) \\psi_i(y) \\, dy$:\n$$ (T_k f)(x) = \\sum_{i=1}^\\infty \\lambda_i \\left( \\int_a^b f(y) \\psi_i(y) \\, dy \\right) \\psi_i(x) $$\n\nSince both expressions represent the same function $(T_k f)(x)$, we can equate them:\n$$ \\int_a^b k(x,y) f(y) \\, dy = \\sum_{i=1}^\\infty \\lambda_i \\left( \\int_a^b f(y) \\psi_i(y) \\, dy \\right) \\psi_i(x) $$\n\n**Step 2: Rearrange the summation and integral on the right-hand side**\n\nThe right-hand side has a summation outside an integral. We want to combine this into a single integral to make it easier to compare with the left-hand side. Given that the series converges absolutely and uniformly (a result established in Step 4 of Mercer's proof, which justifies this manipulation), we can interchange the order of summation and integration:\n\n$$ \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\left( \\int_a^b f(y) \\psi_i(y) \\, dy \\right) $$\nWe can move $\\lambda_i \\psi_i(x)$ inside the integral, as it does not depend on $y$:\n$$ = \\sum_{i=1}^\\infty \\int_a^b \\lambda_i \\psi_i(x) \\psi_i(y) f(y) \\, dy $$\nNow, we can move the summation inside the integral:\n$$ = \\int_a^b \\left( \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y) \\right) f(y) \\, dy $$\n\n**Step 3: Equate the integrands (the \"ignoring $f$\" step)**\n\nNow we have:\n$$ \\int_a^b k(x,y) f(y) \\, dy = \\int_a^b \\left( \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y) \\right) f(y) \\, dy $$\nRearranging, we get:\n$$ \\int_a^b \\left[ k(x,y) - \\left( \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y) \\right) \\right] f(y) \\, dy = 0 $$\nThis equation must hold for *any* arbitrary function $f(y) \\in L^2(a,b)$. A fundamental result in functional analysis (specifically, the Fundamental Lemma of Calculus of Variations or properties of $L^2$ spaces) states that if the integral of a function multiplied by *any* test function is zero, then the function itself must be zero almost everywhere.\n\nTherefore, for each fixed $x$, the term in the square brackets must be zero for almost all $y \\in [a,b]$:\n$$ k(x,y) - \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y) = 0 \\quad \\text{for almost all } y \\in [a,b] $$\nSince the kernel $k(x,y)$ is assumed to be continuous, and the sum $\\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y)$ is also a continuous function (due to the uniform convergence established in Step 4 of the proof), an equality that holds \"almost everywhere\" for continuous functions implies it holds \"everywhere.\"\n\nThus, we arrive at Equation (27):\n$$ k(x,y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y) $$\nThis completes the derivation of the Mercer's series expansion for the kernel function, showing how it can be represented as a sum of products of its eigenfunctions weighted by their corresponding eigenvalues."
    },
    {
        "role": "user",
        "msg": "Output the proof of Mercer's Theorem as Latex code, including the complement in this conversation."
    },
    {
        "role": "assistant",
        "msg": "```latex\n\\documentclass{article}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{amsthm}\n\\usepackage[numbers]{natbib}\n\\usepackage{hyperref}\n\n% Define theorem and definition environments\n\\newtheorem{definition}{Definition}[section]\n\\newtheorem{theorem}{Theorem}[section]\n\\newtheorem{corollary}{Corollary}[section]\n\\newtheorem{lemma}{Lemma}[section]\n\\newtheorem{remark}{Remark}[section]\n\n% Custom command for references if not using a full .bib file\n% For the purpose of this output, we'll assume a .bib is available\n% or these citations are just text. If a full compilation is needed,\n% a .bib file with the entries from the paper would be required.\n\n\\begin{document}\n\n\\section{Mercer\u2019s Theorem and Feature Map} \\label{section:mercers_theorem_and_feature_map}\n\n\\subsection{Mercer\u2019s Theorem} \\label{section:mercers_theorem}\n\n\\begin{definition}[De\ufb01nite Kernel (\\cite{Hilbert, 1904})] \\label{definition:definite_kernel}\nThe function $k : [a, b] \\times [a, b] \\rightarrow \\mathbb{R}$ is a de\ufb01nite kernel where the following double integral:\n$$ J(f) = \\int_{a}^{b} \\int_{a}^{b} k(x, y) f(x) f(y) dx dy, $$\nsatis\ufb01es $J(f) > 0$ for all $f(x) \\neq 0$.\n\\end{definition}\n\nMercer improved over Hilbert\u2019s work (\\cite{Hilbert, 1904}) to propose his theorem, the Mercer\u2019s theorem (\\cite{Mercer, 1909}), introduced in the following.\n\n\\begin{theorem}[Mercer\u2019s Theorem (\\cite{Mercer, 1909})] \\label{theorem:mercers_theorem_main}\nSuppose $k : [a, b] \\times [a, b] \\rightarrow \\mathbb{R}$ is a continuous symmetric positive semi-de\ufb01nite kernel which is bounded:\n$$ \\sup_{x, y} k(x, y) < \\infty. \\label{equation:mercer_bounded} $$\nAssume the operator $T_k$ takes a function $f(x)$ as its argument and outputs a new function as:\n$$ T_k f(x) := \\int_{a}^{b} k(x, y) f(y) dy, \\label{equation:mercer_operator} $$\nwhich is a Fredholm integral equation (\\cite{Schmidt, 1908}). The operator $T_k$ is called the Hilbert\u2013Schmidt integral operator (\\cite{Renardy & Rogers, 2006, Chapter 8}). This output function is positive semi-de\ufb01nite:\n$$ \\iint k(x, y) f(y) dx dy \\geq 0. \\label{equation:mercer_positive_semidefinite_integral} $$\nThen, there is a set of orthonormal bases $\\{ \\psi_i(.) \\}_{i=1}^\\infty$ of $L^2(a, b)$ consisting of eigenfunctions of $T_K$ such that the corresponding sequence of eigenvalues $\\{ \\lambda_i \\}_{i=1}^\\infty$ are non-negative:\n$$ \\int k(x, y) \\psi_i(y) dy = \\lambda_i \\psi_i(x). \\label{equation:mercer_eigenfunction_decomposition} $$\nThe eigenfunctions corresponding to the non-zero eigenvalues are continuous on $[a, b]$ and $k$ can be represented as (\\cite{Aizerman et al., 1964}):\n$$ k(x, y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y), \\label{equation:mercer_kernel_decomposition} $$\nwhere the convergence is absolute and uniform.\n\\end{theorem}\n\n\\begin{proof}\nA roughly high-level proof for the Mercer\u2019s theorem is as follows.\n\n\\textbf{Step 1 of proof:} According to assumptions of theorem, the Hilbert-Schmidt integral operator $T_k$ is a symmetric operator on $L^2(a, b)$ space. Consider a unit ball in $L^2(a, b)$ as input to the operator. As the kernel is bounded, $\\sup_{x, y} k(x, y) < \\infty$, the sequence $f_1, f_2, \\dots$ converges in norm, i.e. $ \\|f_n - f\\| \\rightarrow 0 $ as $n \\rightarrow 0$. Therefore, according to the Arzel\\`a-Ascoli theorem (\\cite{Arzel\\`a, 1895}), the image of the unit ball after applying the operator is compact. In other words, the operator $T_k$ is compact.\n\n\\textbf{Step 2 of proof:} According to the spectral theorem (\\cite{Hawkins, 1975}), there exist several orthonormal bases $\\{ \\psi_i(.) \\}_{i=1}^\\infty$ in $L^2(a, b)$ for the compact operator $T_k$. This provides a spectral (or eigenvalue) decomposition for the operator $T_k$ (\\cite{Ghojogh et al., 2019a}):\n$$ T_k \\psi_i(x) = \\lambda_i \\psi_i(x), \\label{equation:spectral_decomposition_operator} $$\nwhere $\\{ \\psi_i(.) \\}_{i=1}^\\infty$ and $\\{ \\lambda_i \\}_{i=1}^\\infty$ are the eigenvectors and eigenvalues of the operator $T_k$, respectively. Noticing the de\ufb01ned Eq. \\eqref{equation:mercer_operator} and the eigenvalue decomposition, Eq. \\eqref{equation:spectral_decomposition_operator}, we have:\n$$ \\int k(x, y) \\psi_i(y) dy \\overset{\\eqref{equation:mercer_operator}}{=} T_k \\psi_i(x) \\overset{\\eqref{equation:spectral_decomposition_operator}}{=} \\lambda_i \\psi_i(x). \\label{equation:eigenfunction_decomposition_operator} $$\nThis proves the Eq. \\eqref{equation:mercer_eigenfunction_decomposition} which is the eigenfunction decomposition of the operator $T_k$. Note that the eigenvectors $\\{ \\psi_i(.) \\}_{i=1}^\\infty$ are referred to as the \\textit{eigenfunctions} because the decomposition is applied on a function or operator rather than a matrix. Note that eigenfunctions will be explained more in Section \\ref{section:eigenfunctions}.\n\n\\textbf{Step 3 of proof:} According to Parseval\u2019s theorem (\\cite{Parseval des Chenes, 1806}), the Bessel\u2019s inequality can be converted to equality (\\cite{Saxe, 2002}). For the orthonormal bases $\\{ \\psi_i(.) \\}_{i=1}^\\infty$ in the Hilbert space $\\mathcal{H}$ associated with kernel $k$, we have for any function $f \\in L^2(a, b)$:\n$$ f = \\sum_{i=1}^\\infty \\langle f, \\psi_i \\rangle_k \\psi_i. \\label{equation:parsevals_theorem} $$\nIf we replace $\\psi_i$ with $f$ in Eq. \\eqref{equation:spectral_decomposition_operator} and consider Eq. \\eqref{equation:parsevals_theorem}, we will have:\n$$ T_k f = \\sum_{i=1}^\\infty \\lambda_i \\langle f, \\psi_i \\rangle_k \\psi_i. \\label{equation:tk_f_lambda_psi} $$\nOne can consider Eq. \\eqref{equation:mercer_operator} as $T_k f = kf$. Noticing this and Eq. \\eqref{equation:tk_f_lambda_psi} results in:\n$$ kf = \\sum_{i=1}^\\infty \\lambda_i \\langle f, \\psi_i \\rangle_k \\psi_i. \\label{equation:kf_lambda_psi} $$\nIgnoring $f$ from Eq. \\eqref{equation:kf_lambda_psi} gives:\n$$ k(x, y) = \\sum_{i=1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y), \\label{equation:mercer_kernel_decomposition_reproved} $$\nwhich is Eq. \\eqref{equation:mercer_kernel_decomposition}; hence, that is proved.\n\n\\textbf{Step 4 of proof:} We de\ufb01ne the truncated kernel $r_n$ (with parameter $n$) as:\n$$ r_n(x, y) := k(x, y) - \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(y) = \\sum_{i=n+1}^\\infty \\lambda_i \\psi_i(x) \\psi_i(y). \\label{equation:truncated_kernel} $$\nAs $T_k$ is an integral operator, this truncated kernel has positive kernel, i.e., for every $x \\in [a, b]$, we have:\n$$ r_n(x, x) = k(x, x) - \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(x) \\geq 0 $$\n$$ \\Rightarrow \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(x) \\leq k(x, x) \\leq \\sup_{x \\in [a, b]} k(x, x). \\label{equation:truncated_kernel_bound} $$\nBy Cauchy-Schwartz inequality, we have:\n$$ \\left| \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(y) \\right|^2 \\leq \\left( \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(x) \\right) \\left( \\sum_{i=1}^n \\lambda_i \\psi_i(y) \\psi_i(y) \\right) $$\n$$ \\overset{\\eqref{equation:truncated_kernel_bound}}{\\leq} \\left( \\sup_{x \\in [a, b]} k(x, x) \\right)^2. $$\nTaking second root from the sides of inequality gives:\n$$ \\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(x) \\leq \\sup_{x \\in [a, b]} |k(x, x)| \\overset{\\eqref{equation:mercer_bounded}}{\\leq} \\infty. \\label{equation:mercer_convergence} $$\nThis shows that the sequence $\\sum_{i=1}^n \\lambda_i \\psi_i(x) \\psi_i(x)$ converges absolutely and uniformly.\n\\end{proof}\n\nThe Mercer\u2019s theorem is very important. Many of its equations, such as Eqs. \\eqref{equation:mercer_operator}, \\eqref{equation:mercer_eigenfunction_decomposition}, and \\eqref{equation:mercer_kernel_decomposition} are used in theory of kernels and kernel methods.\n\n\\subsection{Feature Map and Pulling Function} \\label{section:feature_map}\nLet $\\mathcal{X} := \\{x_i\\}_{i=1}^n$ be the set of data in the input space (note that the input space is the original space of data). The $t$-dimensional (perhaps in\ufb01nite dimensional) feature space (or Hilbert space) is denoted by $\\mathcal{H}$.\n\n\\begin{definition}[Feature Map or Pulling Function] \\label{definition:feature_map}\nWe de\ufb01ne the mapping:\n$$ \\phi : \\mathcal{X} \\rightarrow \\mathcal{H}, \\label{equation:feature_map_definition} $$\nto transform data from the input space to the feature space, i.e. Hilbert space. In other words, this mapping pulls data to the feature space:\n$$ x \\mapsto \\phi(x). \\label{equation:pulling_function} $$\nThe function $\\phi(x)$ is called the feature map or pulling function. The feature map is a (possibly in\ufb01nite-dimensional) vector whose elements are (\\cite{Minh et al., 2006}):\n$$ \\phi(x) = [\\phi_1(x), \\phi_2(x), \\dots]^\\top := [\\sqrt{\\lambda_1} \\psi_1(x), \\sqrt{\\lambda_2} \\psi_2(x), \\dots]^\\top, \\label{equation:feature_map_elements} $$\nwhere $\\{ \\psi_i \\}$ and $\\{ \\lambda_i \\}$ are eigenfunctions and eigenvalues of the kernel operator (see Eq. \\eqref{equation:mercer_eigenfunction_decomposition}). Note that eigenfunctions will be explained more in Section \\ref{section:eigenfunctions}.\n\\end{definition}\n\nLet $t$ denote the dimensionality of $\\phi(x)$. The feature map may be in\ufb01nite or \ufb01nite dimensional, i.e. $t$ can be in\ufb01nity; it is usually a very large number (recall De\ufb01nition \\ref{definition:hilbert_space} where we said Hilbert space may have in\ufb01nite number of dimensions).\n\nConsidering both Eqs. \\eqref{equation:mercer_kernel_decomposition} and \\eqref{equation:feature_map_elements} shows that:\n$$ k(x, y) = \\langle \\phi(x), \\phi(y) \\rangle_k = \\phi(x)^\\top \\phi(y). \\label{equation:kernel_inner_product} $$\nHence, the kernel between two points is the inner product of pulled data points to the feature space. Suppose we stack the feature maps of all points $\\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ column-wise in:\n$$ \\mathbf{\\Phi}(\\mathbf{X}) := [\\phi(x_1), \\phi(x_2), \\dots, \\phi(x_n)], \\label{equation:stacked_feature_maps} $$\nwhich is $t \\times n$ dimensional and $t$ may be in\ufb01nity or a large number. The kernel matrix de\ufb01ned in De\ufb01nition \\ref{definition:gram_matrix} can be calculated as:\n$$ \\mathbb{R}^{n \\times n} \\ni \\mathbf{K} = \\langle \\mathbf{\\Phi}(\\mathbf{X}), \\mathbf{\\Phi}(\\mathbf{X}) \\rangle_k = \\mathbf{\\Phi}(\\mathbf{X})^\\top \\mathbf{\\Phi}(\\mathbf{X}). \\label{equation:kernel_matrix_phi} $$\nEqs. \\eqref{equation:kernel_inner_product} and \\eqref{equation:kernel_matrix_phi} show that there is no need to compute kernel using eigenfunctions but a simple inner product suf\ufb01ces for kernel computation. This is the beauty of kernel methods which are simple to compute.\n\n\\end{document}\n```"
    }
]